INFO:     Started server process [194463]
INFO:     Waiting for application startup.
INFO:main:Starting SLM API server...
INFO:main:SmolLM model DISABLED to save GPU memory
INFO:main:DeepSeek model DISABLED to save GPU memory
INFO:main:Nemotron model DISABLED to save GPU memory
INFO:main:Jamba model DISABLED to save GPU memory
INFO:main:Loading Phi model...
INFO:main:Using device: cuda
INFO:main:Loading microsoft/Phi-3-mini-128k-instruct...
`torch_dtype` is deprecated! Use `dtype` instead!
WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_mini_hyphen_128k_hyphen_instruct.072cb7562cb8c4adf682a8e186aaafa49469eb5d.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_mini_hyphen_128k_hyphen_instruct.072cb7562cb8c4adf682a8e186aaafa49469eb5d.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
INFO:main:Phi model loaded successfully on cuda
INFO:main:Model vocab size: 32064
INFO:main:Model context length: 128K tokens (131,072 tokens)
INFO:main:Phi model loaded successfully!
INFO:main:SLM API server ready with Phi model only!
INFO:     Application startup complete.
ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use
INFO:     Waiting for application shutdown.
INFO:main:Shutting down SLM API server...
INFO:     Application shutdown complete.
